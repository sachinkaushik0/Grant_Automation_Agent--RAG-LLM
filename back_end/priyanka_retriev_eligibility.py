# -*- coding: utf-8 -*-
"""eligibility_check.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VShWVltNTZKUBwt-qmzDzGAX4DUnRlq3
"""

!pip uninstall -y cupy cupy-cuda11x cupy-cuda12x

!pip list | grep cupy

!pip show cupy

!nvidia-smi

!pip install cupy-cuda12x

!pip install spacy
!pip install spacy transformers sentence-transformers
!python -m spacy download en_core_web_sm

!pip install langdetect

import json
import re
import spacy
import pandas as pd
from langdetect import detect, DetectorFactory
from collections import Counter
from transformers import pipeline
import matplotlib.pyplot as plt

# Ensure consistent language detection results
DetectorFactory.seed = 0

# Load English NLP model for Named Entity Recognition
nlp = spacy.load("en_core_web_sm")

# Load Llama or a similar question-answering model
qa_model = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Load JSON Data, handling invalid control characters
def load_json_with_control_char_fix(filepath):
    with open(filepath, "r", encoding="utf-8") as file:
        content = file.read()
        content = re.sub(r"[\u0000-\u001F]+", " ", content)  # Replace invalid control characters
        try:
            json_objects = json.loads(content)  # Load JSON properly
            return json_objects
        except json.JSONDecodeError as e:
            print(f"Error decoding JSON: {e}")
            return []

# Define error patterns
error_patterns = [
    r"Your browser does not support JavaScript",
    r"Some components may not be visible",
    r"Please enable JavaScript",
    r"Error loading page",
    r"This content is not available",
]

# Define classification keywords
individual_keywords = ["individual", "entrepreneur", "farmer", "student", "researcher", "self-employed", "artist", "freelancer", "scholarship"]
institution_keywords = ["business", "company", "corporation", "organization", "startup", "nonprofit", "institution", "university", "SME", "NGO","local retailers","livestock partners","lending limits","Competitive rates"]

# Function to detect language
def detect_language(text):
    try:
        return detect(text)
    except:
        return "unknown"

# Function to check if text contains an error message
def contains_error_message(text):
    return any(re.search(pattern, text, re.IGNORECASE) for pattern in error_patterns)

# Function to classify grants
def classify_grant(grant):
    """Determine if the grant is for an individual or institution based on keywords."""
    text = grant.get("full_text", "") or grant.get("description", "")

    if not text or not text.strip():  # Check if text is empty or None
        return "Unknown"

    text_lower = text.lower()

    # Handle None case before calling .lower()
    audience = grant.get("target_audience", "")
    if audience is None:
        audience = ""  # Ensure it's a valid string
    else:
        audience = audience.lower()

    if any(keyword in text_lower for keyword in individual_keywords) or any(keyword in audience for keyword in individual_keywords):
        return "Individual"

    if any(keyword in text_lower for keyword in institution_keywords) or any(keyword in audience for keyword in institution_keywords):
        return "Institution"

    return "Unknown"


# Function to clean and filter data
def clean_and_filter_data(grants):
    language_counts = Counter()
    error_count = 0
    null_text_count = 0
    cleaned_grants = []

    for grant in grants:
        text = grant.get("full_text", None) or grant.get("description", None)

        # Handle None before calling .strip()
        if text is None:
            text = ""

        if not text.strip():  # Safe check for empty values
            null_text_count += 1
            continue  # Skip empty entries

        detected_lang = detect_language(text)
        language_counts[detected_lang] += 1

        if contains_error_message(text):
            error_count += 1
            continue  # Skip error entries

        if detected_lang == "en":
            grant["recipient_type"] = classify_grant(grant)  # Assign classification
            cleaned_grants.append(grant)

    return cleaned_grants, language_counts, error_count, null_text_count


# Load JSON file
json_file_path = "/content/sample_data/isc_funding_openai_PROGRAMS.json"
grants = load_json_with_control_char_fix(json_file_path)

# Ensure grants are loaded
if not grants or not isinstance(grants, list):
    raise ValueError("Error: No grants data found or JSON structure is incorrect.")

# Apply classification to all grants
for grant in grants:
    grant["recipient_type"] = classify_grant(grant)

# Process grants data
cleaned_grants, language_counts, error_count, null_text_count = clean_and_filter_data(grants)

# Convert language counts to DataFrame
language_counts_df = pd.DataFrame(language_counts.items(), columns=["Language", "Count"])

# Count grants per recipient type
recipient_counts = Counter(grant["recipient_type"] for grant in cleaned_grants)

# Convert to DataFrame for visualization
recipient_df = pd.DataFrame(recipient_counts.items(), columns=["Recipient Type", "Count"])

# Plot bar chart for grant classification
plt.figure(figsize=(8, 6))
plt.bar(recipient_df["Recipient Type"], recipient_df["Count"], color=['blue', 'green', 'gray'])
plt.xlabel("Recipient Type")
plt.ylabel("Count")
plt.title("Total Count of Individual vs Institutional Grants")
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Save cleaned data to JSON
with open("cleaned_grants_with_recipient_type.json", "w", encoding="utf-8") as file:
    json.dump(cleaned_grants, file, indent=4, ensure_ascii=False)

# Display detected language counts
print("Detected Language Counts:")
print(language_counts_df)

# Print summary of removed entries
print(f"üö® Error messages detected and removed: {error_count}")
print(f"üö® Null or empty 'full_text' and 'description' entries removed: {null_text_count}")
print("‚úÖ Cleaned data saved to 'cleaned_grants_with_recipient_type.json'.")

# Example Usage (Checking eligibility for the first 3 grants)
for grant in cleaned_grants[:3]:
    print(f"\nChecking eligibility for: {grant['program_name']}")
    print(f"Grant is for: {grant['recipient_type']}\n")  # Show classification

print("Grant Classification Count:")
display(recipient_df) # Using pandas' display function
# or
print(recipient_df.head()) # Using head() to display the first few rows
plt.show()

# Filter out grants classified as "Unknown"
unknown_grants = [grant for grant in cleaned_grants if grant["recipient_type"] == "Unknown"]

# Convert to DataFrame for better visualization
unknown_df = pd.DataFrame(unknown_grants, columns=["program_id", "program_name", "description"])

# Print unknown classification details
print("\nüö® Grants classified as 'Unknown':")
if not unknown_df.empty:
    print(unknown_df.head(20))  # Print first 20 for quick review
else:
    print("‚úÖ No grants classified as 'Unknown'.")

# Save unknown grants to a separate JSON file
with open("unknown_grants.json", "w", encoding="utf-8") as file:
    json.dump(unknown_grants, file, indent=4, ensure_ascii=False)

print("\n‚úÖ Unknown grants saved to 'unknown_grants.json'.")

import json
import re
import spacy
import pandas as pd
from langdetect import detect, DetectorFactory
from collections import Counter
from transformers import pipeline

# Ensure consistent language detection results
DetectorFactory.seed = 0

# Load English NLP model for Named Entity Recognition
nlp = spacy.load("en_core_web_sm")

# Load Llama or a similar question-answering model
qa_model = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Load JSON Data
import json
import re

# Read and sanitize JSON
with open("/content/sample_data/isc_funding_openai_PROGRAMS.json", "r", encoding="utf-8", errors="replace") as file:
    raw_data = file.read()

# Remove invalid control characters
raw_data = re.sub(r"[\x00-\x1F\x7F\n\t\x00]", " ", raw_data)  # Replaces control chars with space

# Try loading JSON again
try:
    grants = json.loads(raw_data)
    print("‚úÖ JSON successfully loaded after sanitization!")
except json.JSONDecodeError as e:
    print(f"‚ùå Error: {e}")


# Define common error messages or patterns
error_patterns = [
    r"Your browser does not support JavaScript",
    r"Some components may not be visible",
    r"Please enable JavaScript",
    r"Error loading page",
    r"This content is not available",
]

# Keywords for classification
individual_keywords = ["individual", "entrepreneur", "farmer", "student", "researcher", "self-employed", "artist", "freelancer", "scholarship"]
organization_keywords = ["business", "company", "corporation", "organization", "startup", "nonprofit", "institution", "university", "SME", "NGO"]

# Function to detect language
def detect_language(text):
    try:
        return detect(text)
    except:
        return "unknown"

# Function to check if text contains an error message
def contains_error_message(text):
    for pattern in error_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            return True
    return False


# Function to classify a grant as Individual or Organization
#def classify_grant(grant):
#   """Determine if the grant is for an Individual or Organization based on keywords."""
 #   text = grant.get("full_text", "") or grant.get("description", "")
 #   text_lower = text.lower()

 #
 #   if any(keyword in text_lower for keyword in individual_keywords):
 #       return "Individual"

 #   if any(keyword in text_lower for keyword in organization_keywords):
#        return "Organization"

#    return "Unknown"

def classify_grant(grant):
    """Determine if the grant is for an individual or institution using Named Entity Recognition (NER)."""
    text = grant.get("full_text", "") or grant.get("description", "")

    if not text or not text.strip():
        return "Unknown"

    doc = nlp(text)  # Apply NLP model for entity recognition

    individual_score = 0
    institution_score = 0

    for ent in doc.ents:
        if ent.label_ == "PERSON":  # Likely referring to an individual
            individual_score += 1
        elif ent.label_ in ["business", "company", "corporation", "organization", "startup", "nonprofit", "institution", "university", "SME", "NGO"]:  # Likely referring to an institution
            institution_score += 1

    # Classify grant based on entity counts
    if individual_score > institution_score:
        return "Individual"
    elif institution_score > individual_score:
        return "Institution"

    return "Unknown"


# Function to clean and filter data
def clean_and_filter_data(grants):
    language_counts = Counter()
    error_count = 0
    null_text_count = 0
    non_english_ids = []
    error_message_ids = []
    null_text_ids = []

    cleaned_grants = []

    for grant in grants:
        # Ensure full_text and description are strings
        text = str(grant.get("full_text", "") or grant.get("description", "") or "").strip()
        program_id = grant.get("program_id", "Unknown")

        # Remove grants with null or empty full_text & description
        if not text:
            null_text_count += 1
            null_text_ids.append(program_id)
            continue  # Skip this entry

        detected_lang = detect_language(text)
        language_counts[detected_lang] += 1

        # Remove non-English grants
        if detected_lang != "en":
            non_english_ids.append(program_id)
            continue  # Skip this entry

        # Remove grants containing error messages
        if contains_error_message(text):
            error_count += 1
            error_message_ids.append(program_id)
            continue  # Skip this entry

        # Classify grants
        grant["recipient_type"] = classify_grant(grant)

        # Add clean grants
        cleaned_grants.append(grant)

    return cleaned_grants, language_counts, error_count, null_text_count, non_english_ids, error_message_ids, null_text_ids

# Function to clean and preprocess text
def preprocess_text(text):
    """Remove special characters and redundant spaces."""
    text = re.sub(r"\s+", " ", text)  # Normalize spaces
    text = re.sub(r"[^a-zA-Z0-9,.%$-]", " ", text)  # Keep relevant characters
    return text.strip()

# Function to extract eligibility criteria using Named Entity Recognition (NER) & Regex
def extract_eligibility(grant):
    """Extracts eligibility criteria from full_text or description field."""
    text = grant.get("full_text", "") or grant.get("description", "")
    cleaned_text = preprocess_text(text)

    # Apply NER model
    doc = nlp(cleaned_text)

    # Extract relevant entities
    eligibility_criteria = {
        "location": None,
        "industry": None,
        "business_type": None,
        "employee_count": None,
        "revenue": None,
        "specific_requirements": [],
    }

    for ent in doc.ents:
        if ent.label_ in ["GPE", "LOC"]:
            eligibility_criteria["location"] = ent.text
        elif ent.label_ == "ORG":
            eligibility_criteria["business_type"] = ent.text
        elif ent.label_ == "MONEY":
            eligibility_criteria["revenue"] = ent.text
        elif ent.label_ == "CARDINAL":
            if "employees" in cleaned_text.lower():
                eligibility_criteria["employee_count"] = ent.text

    # Use regex to extract additional details
    specific_requirements = re.findall(
        r"(must provide|requirement[s]? include|eligible if|to qualify, you need) (.*?)[.!?]",
        cleaned_text,
        re.IGNORECASE,
    )

    if specific_requirements:
        eligibility_criteria["specific_requirements"] = [req[1] for req in specific_requirements]

    return eligibility_criteria

# Function to generate eligibility questions using LLM
def generate_questions(eligibility_criteria):
    """Generate interactive eligibility questions from extracted criteria."""
    questions = []

    if eligibility_criteria["location"]:
        questions.append(f"Are you located in {eligibility_criteria['location']}?")

    if eligibility_criteria["industry"]:
        questions.append(f"Does your business belong to the {eligibility_criteria['industry']} sector?")

    if eligibility_criteria["business_type"]:
        questions.append(f"Is your business type {eligibility_criteria['business_type']}?")

    if eligibility_criteria["employee_count"]:
        questions.append(f"Does your company have {eligibility_criteria['employee_count']} employees?")

    if eligibility_criteria["revenue"]:
        questions.append(f"Is your company's revenue around {eligibility_criteria['revenue']}?")

    for req in eligibility_criteria["specific_requirements"]:
        questions.append(f"Do you meet this requirement: '{req}'?")

    return questions

def check_user_eligibility(grant):
    """Interacts with user to validate eligibility for a given grant."""
    #print(f"\nChecking eligibility for: {grant['program_name']}")
    #print(f"Grant is for: {grant['recipient_type']}\n")  # Show classification (Individual/Organization)

    # Extract eligibility criteria
    eligibility_criteria = extract_eligibility(grant)

    # Generate interactive questions
    questions = generate_questions(eligibility_criteria)

    # If no questions are generated, print a message and return
    if not questions:
        print("‚ö†Ô∏è No eligibility questions generated for this grant.")
        return False

    user_responses = {}

    for question in questions:
        response = input(f"{question} (yes/no): ").strip().lower()
        user_responses[question] = response

    # Summarizing eligibility results
    eligible = all(response == "yes" for response in user_responses.values())

    if eligible:
        print("\n‚úÖ You are eligible for this grant! Proceeding to the application stage.")
    else:
        print("\n‚ùå Unfortunately, you do not meet all eligibility requirements.")

    return eligible


# Process grants data
cleaned_grants, language_counts, error_count, null_text_count, non_english_ids, error_message_ids, null_text_ids = clean_and_filter_data(grants)

# Convert language counts to a DataFrame for visualization
language_counts_df = pd.DataFrame(language_counts.items(), columns=["Language", "Count"])

# Save cleaned data to JSON (optional)
with open("cleaned_grants_with_recipient_type.json", "w", encoding="utf-8") as file:
    json.dump(cleaned_grants, file, indent=4, ensure_ascii=False)

# Display detected language counts
print("\nDetected Language Counts:")
print(language_counts_df)

# Print summary of removed entries
print(f"\nüö® Error messages detected and removed: {error_count}")
print(f"Removed Program IDs: {error_message_ids}")

print(f"\nüö® Non-English grants removed: {len(non_english_ids)}")
print(f"Removed Program IDs: {non_english_ids}")

print(f"\nüö® Null or empty 'full_text' and 'description' entries removed: {null_text_count}")
print(f"Removed Program IDs: {null_text_ids}")

print("\n‚úÖ Cleaned data saved to 'cleaned_grants_with_recipient_type.json'.")

# Example Usage (Checking eligibility for the first 3 grants)

for grant in cleaned_grants[:3]:
    print(f"\nChecking eligibility for: {grant['program_name']}")
    print(f"Grant is for: {grant['recipient_type']}\n")  # Show classification

    # Call the eligibility check function here
    check_user_eligibility(grant)

for grant in cleaned_grants[3:8]:
    print(f"\nChecking eligibility for: {grant['program_name']}")
    print(f"Grant is for: {grant['recipient_type']}\n")  # Show classification

    # Call the eligibility check function here
    check_user_eligibility(grant)